{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Web scraping - jak zdrapywać dane?</h1>\n",
    "\n",
    "<h3 align=\"center\" style=\"font-family: 'Arial'\">   Paweł Bojanowski</h3>\n",
    "<img src=\"images/pylight.png\" align=\"center\" width=\"15%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"font-family: 'Arial'\">Paweł Bojanowski</h3>\n",
    "<img src=\"images/saucebot.jpg\" width=\"30%\" align=\"right\"/>\n",
    "\n",
    " * skończyłem geoinformatykę na Uniwersytecie Warszawskim\n",
    " \n",
    " * 2 lata pracowałem jako Web Backend Dev\n",
    " \n",
    " * akutalnie Sauce Labs (chmury, mikroserwisy, trochę DevOpsowania)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\"> Czym jest web scraping i do czego jest używany?</h2>\n",
    "<br><br><br>\n",
    "<div>\n",
    "    <ul>\n",
    "        <li>Web scraping jest techiniką automatycznej ekstracji danych ze stron internetowych.</li>\n",
    "        <li>Pozyskiwanie danych kontaktowych</li>\n",
    "        <li>Pozyskiwanie ocen użytkowników</li>\n",
    "    </ul>\n",
    "        </div>\n",
    "        <img src=\"images/boromir.jpg\" align=\"right\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Web scraping - moja historia</h1>\n",
    "<img src=\"images/codingbot.gif\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Tutaj historia o tym jak szukałem mieszkania na wynajem i żeby \n",
    "codziennie nie przeglądać po 100 razy 3 portali z ogłoszeniami, napisałem program, który co 5 minut sprawdzał czy \n",
    "pojawiły się nowe ogłoszenia w interesujących mnie kryteriach i dawał mi znać, abym mógł być pierwszym zainteresowanym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Webscraping - użyteczne biblioteki</h1>\n",
    "\n",
    "* [Requests](https://2.python-requests.org//pl/latest/) <- żeby pobrać kod źródłowy HTML strony\n",
    "* [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc) <- żeby sparsować źródłowy HTML\n",
    "* [Selenium](https://selenium-python.readthedocs.io/) <- kiedy musimy wejść w interakcję z przeglądarką (np. nacisnąć przycisk na stronie, albo zescrollować)\n",
    "* [Requests-HTML](http://html.python-requests.org/) <- Requests + BeautifulSoup in pythonic way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/pylight-website.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Załóżmy, że co miesiąc z niecierpliwością czekacie na wiadomość o nowym PyLightcie, a nie chcecie tracić czasu na codzienne sprawdzanie, gdzie i kiedy się odbędzie oraz o czym będą prezentację. Spróbujmy sobie to zautomatyzować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    # url: np. https://www.pylight.org\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/when.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def check_date_and_address(response):\n",
    "    # find first HTML element with class date-and-localization\n",
    "    details = response.html.find(\".date-and-localization\", first=True)\n",
    "    return details.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/talk.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_talks(response):\n",
    "    # Find all elements with class talk-detail\n",
    "    talks = response.html.find(\".talk-detail\")\n",
    "    return talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wtorek, 18.06.2019, godz. 18:30\n",
      "Sumo Logic, al. Solidarności 173, budynek C, piętro 6\n",
      "» WIĘCEJ INFORMACJI\n",
      "\n",
      "\n",
      "Ignacy Janiszewski\n",
      "Uczenie maszynowe dla pyczątkujących\n",
      "\n",
      "\n",
      "Paweł Bojanowski\n",
      "Web scraping - jak zdrapywać dane?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    session = HTMLSession()\n",
    "    response = session.get(url)\n",
    "    return response\n",
    "\n",
    "\n",
    "def check_date_and_address(response):\n",
    "    details = response.html.find(\".date-and-localization\", first=True)\n",
    "    return details.text\n",
    "\n",
    "def get_talks(response):\n",
    "    talks = response.html.find(\".talk-detail\")\n",
    "    return talks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response = get_html(url=\"https://www.pylight.org\")\n",
    "    meetup_details = check_date_and_address(response)\n",
    "    talks = get_talks(response)\n",
    "    print(meetup_details)\n",
    "    print(\"\\n\")\n",
    "    for talk in talks:\n",
    "        print(talk.text)\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Gimme moar!</h1>\n",
    "<br><br>\n",
    "<img src=\"images/issues.png\" width=\"60%\" align=\"left\"/>\n",
    "<img src=\"images/qrcode.png\" align=\"right\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Otworzyłem repozytorium z kodem z prezentacji na githubie, jednak kod ma pewne defekty. \n",
    "Jeśli ktoś z uczestników chciałby się nimi zająć i przygotować pull requesty z poprawkami, chętnie zrobię code review!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
